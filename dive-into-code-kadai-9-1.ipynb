{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tixt1\\Desktop\\機械学習特別研修\\dive-into-code-kadai-9-1.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tixt1/Desktop/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%89%B9%E5%88%A5%E7%A0%94%E4%BF%AE/dive-into-code-kadai-9-1.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# データセットをダウンロード\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tixt1/Desktop/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%89%B9%E5%88%A5%E7%A0%94%E4%BF%AE/dive-into-code-kadai-9-1.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m mnist\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tixt1/Desktop/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%89%B9%E5%88%A5%E7%A0%94%E4%BF%AE/dive-into-code-kadai-9-1.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m (X, y), (X_test, y_test) \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39mload_data()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# データセットをダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X, y), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データチェック\n",
    "print(X.shape) # (60000, 28, 28)\n",
    "print(X.shape) # (10000, 28, 28)\n",
    "print(X[0].dtype) # uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平滑化\n",
    "X_flat = X.reshape(-1, 784)\n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "print(X_flat.shape)\n",
    "print(X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 型変換、正規化\n",
    "X_flat = X_flat.astype(np.float)\n",
    "X_test_flat = X_test_flat.astype(np.float)\n",
    "X_flat /= 255\n",
    "X_test_flat /= 255\n",
    "print(X_flat.max()) # 1.0\n",
    "print(X_flat.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正しいラベル値のワンホット エンコーディング\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y.shape) # (60000,)\n",
    "print(y_one_hot.shape) # (60000, 10)\n",
    "print(y_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング データと検証データに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_flat, y_one_hot, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch Deep Neural Network (今のところ、3層 NN)\n",
    "class ScratchDeepNeuralNetworkClassifier():\n",
    "    \"\"\"\n",
    "    N層ニューラル ネットワーク分類器\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.sigma : ガウス分布の標準偏差\n",
    "    self.lr : 学習率\n",
    "    self.n_nodes1 : 1層目のノード数\n",
    "    self.n_nodes2 : 2層目のノード数\n",
    "    self.n_output : 出力層のノード数\n",
    "    \n",
    "    self.n_epoch : エポック数\n",
    "    self.n_batch : バッチ数\n",
    "    self.verbose : 学習プロセスの可視化\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes1, n_nodes2, n_output,\n",
    "                 sigma, n_epoch, n_batch, lr, verbose = False):\n",
    "        # Parameters\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "                \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        \"\"\"\n",
    "        ニューラル ネットワーク分類器をトレーニングする\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            訓練データの特徴\n",
    "        y : 次の形の ndarray, shape (n_samples, )\n",
    "            学習データの正解値\n",
    "        X_val : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            検証データの特徴\n",
    "        y_val : 次の形の ndarray, shape (n_samples, )\n",
    "            検証データの正しい値\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer1 = AdaGrad(self.lr)\n",
    "        optimizer2 = AdaGrad(self.lr)\n",
    "        optimizer3 = AdaGrad(self.lr)\n",
    "        \n",
    "        initializer1 = XavierInitializer()\n",
    "        initializer2 = XavierInitializer()\n",
    "        initializer3 = SimpleInitializer(self.sigma)\n",
    "        \n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1, Tanh())\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2, Tanh())\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3, Softmax())\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            \n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                # フォワードプロパゲーション\n",
    "                self.Z1 = self.FC1.forward(mini_X_train)\n",
    "                self.Z2 = self.FC2.forward(self.Z1)\n",
    "                self.Z3 = self.FC3.forward(self.Z2)    \n",
    "\n",
    "                # バックプロパゲーション\n",
    "                self.dA3 = (self.Z3 - mini_y_train)/self.n_batch\n",
    "                self.dZ2 = self.FC3.backward(self.dA3)\n",
    "                self.dZ1 = self.FC2.backward(self.dZ2)\n",
    "                self.dZ0 = self.FC1.backward(self.dZ1)\n",
    "                \n",
    "                # 損失関数\n",
    "                self.loss += self.loss_function(self.Z3,mini_y_train)\n",
    "                \n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラル ネットワーク分類器を使用して推定\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形の ndarray、形状 (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        pred_Z1 = self.FC1.forward(X)\n",
    "        pred_Z2 = self.FC2.forward(pred_Z1)\n",
    "        return np.argmax(self.FC3.forward(pred_Z2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ処理クラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "        訓練データ\n",
    "    y : 次の形の ndarray、形状 (n_samples, 1) \n",
    "        正しい値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyでの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題1:全結合層のクラス化\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "        \n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.W(self.n_nodes1,self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.A = np.dot(self.X,self.W) + self.B\n",
    "        \n",
    "        return self.activation.forward(self.A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "             後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "             前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = self.activation.backward(dZ)\n",
    "        self.dB = np.mean(dA,axis=0)\n",
    "        self.dW = np.dot(self.X.T,dA)/len(self.X)\n",
    "        dZ = np.dot(dA,self.W.T)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題2:初期化方法のクラス化\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重み\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : バイアス\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題3:最適化手法のクラス化\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.dW\n",
    "        layer.B -= self.lr*layer.dB\n",
    "        \n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題4:活性化関数のクラス化\n",
    "\n",
    "class Tanh():\n",
    "    \"\"\"\n",
    "    Activation function : ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        self.Z = np.tanh(self.A)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        return dZ*(1-self.Z**2)\n",
    "    \n",
    "class Sigmoid():\n",
    "    \"\"\"\n",
    "    Activation function : シグモイド関数\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        self.Z = 1/(1+np.exp(-self.A))\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        return dZ*(1-self.Z)*self.Z\n",
    "    \n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Activation Function : ソフトマックス関数\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        \n",
    "        return np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)),axis=1,keepdims=True)\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題5:ReLUクラスの作成\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    Activation function : ReLU関数\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        return np.maximum(self.A,0)\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        \n",
    "        return np.where(self.A>0,dZ,0)\n",
    "    \n",
    "a = np.array([-1,0,1,9,-1])\n",
    "b = np.array([1,0,-1,9,-1])\n",
    "\n",
    "print(np.where(a<0,0,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題6:重みの初期値\n",
    "\n",
    "class XavierInitializer():\n",
    "    \"\"\"\n",
    "    Xavier で重みを初期化する\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みを初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重み\n",
    "        \"\"\"\n",
    "        return np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : バイアス\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)\n",
    "    \n",
    "class HeInitializer():\n",
    "    \"\"\"\n",
    "    He による重みの初期化\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重み\n",
    "        \"\"\"\n",
    "        return np.random.randn(n_nodes1, n_nodes2)*np.sqrt(2/n_nodes1)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : バイアス\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題7:最適化手法\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        レイヤーの重みとバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前のレイヤーのインスタンス\n",
    "        \"\"\"\n",
    "        self.hW += layer.dW*layer.dW\n",
    "        self.hB = layer.dB*layer.dB\n",
    "    \n",
    "        layer.W -= self.lr*layer.dW/(np.sqrt(self.hW) +1e-7)\n",
    "        layer.B -= self.lr*layer.dB/(np.sqrt(self.hB) +1e-7)\n",
    "        \n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題8:クラスの完成\n",
    "\n",
    "clf = ScratchDeepNeuralNetworkClassifier(n_epoch=5, n_features=784,\n",
    "                                         n_nodes1=400, n_nodes2=200, n_output=10,\n",
    "                                         sigma=0.01, n_batch=100,\n",
    "                                         lr = 0.01, verbose = False)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid,axis=1), y_pred)\n",
    "print('accuracy:{:.3f}'.format(accuracy))\n",
    "\n",
    "# 各エポックの損失関数を可視化する\n",
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "plt.title('LOSS')\n",
    "plt.plot(clf.log_loss,'rs--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題9:学習と推定\n",
    "\n",
    "# スクラッチディープニューラルネットワーク (任意のレイヤーで実行できる場合)\n",
    "class ScratchDNNClassifier():\n",
    "    \"\"\"\n",
    "    N層ニューラルネットワーク分類器\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    self.n_epoch : エポック数\n",
    "    self.n_batch : バッチ数\n",
    "    self.verbose : 学習プロセスの可視化\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, NN, n_epoch, n_batch, verbose = False):\n",
    "        # Parameters\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "        self.NN = NN\n",
    "        \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "    \n",
    "    def accuracy(self,Z,Y):\n",
    "        return accuracy_score(Y,Z)\n",
    "                \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        \"\"\"\n",
    "        ニューラル ネットワーク分類器をトレーニング\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            訓練データの特徴\n",
    "        y : 次の形の ndarray, shape (n_samples, )\n",
    "            学習データの正解値\n",
    "        X_val : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            検証データの特徴\n",
    "        y_val : 次の形の ndarray, shape (n_samples, )\n",
    "            検証データの正しい値\n",
    "        \"\"\"\n",
    "        for epoch in range(self.n_epoch):\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            \n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                # フォワードプロパゲーション\n",
    "                forward_data = mini_X_train\n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "                # 予測値\n",
    "                Z = forward_data\n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                backward_data = (Z - mini_y_train)/self.n_batch\n",
    "                for layer in range(len(self.NN)-1,-1,-1):\n",
    "                    backward_data = self.NN[layer].backward(backward_data)\n",
    "                \n",
    "                # 損失関数\n",
    "                self.loss += self.loss_function(Z,mini_y_train)\n",
    "                \n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            self.log_acc[epoch] = self.accuracy(self.predict(X),np.argmax(y,axis=1))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使用して推定\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形の ndarray、形状 (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        pred_data = X\n",
    "        \n",
    "        for layer in range(len(self.NN)):\n",
    "            pred_data = self.NN[layer].forward(pred_data)\n",
    "            \n",
    "        return np.argmax(pred_data,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2 = {0:FC(784, 400, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       1:FC(400, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       2:FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = ScratchDNNClassifier(NN=NN2,n_epoch=30,n_batch=20,verbose = False)\n",
    "\n",
    "clf2.fit(X_train,y_train)\n",
    "y_pred2 = clf2.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred2)\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid,axis=1), y_pred2)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各エポックの損失関数を可視化\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS')\n",
    "plt.plot(clf2.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACC')\n",
    "plt.plot(clf2.log_acc,'rs--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN3 = {0:FC(784, 400, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       1:FC(400, 600, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       2:FC(600, 300, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       3:FC(300, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = ScratchDNNClassifier(NN=NN3,n_epoch=30,n_batch=20,verbose = False)\n",
    "\n",
    "clf3.fit(X_train,y_train)\n",
    "y_pred3 = clf3.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred3)\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid,axis=1), y_pred3)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各エポックの損失関数を可視化\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS')\n",
    "plt.plot(clf3.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACC')\n",
    "plt.plot(clf3.log_acc,'rs--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN4 = {0:FC(784, 400, XavierInitializer(), AdaGrad(0.01), Tanh()),\n",
    "       1:FC(400, 300, XavierInitializer(), AdaGrad(0.01), Sigmoid()),\n",
    "       2:FC(300, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       3:FC(200, 100, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       4:FC(100, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = ScratchDNNClassifier(NN=NN4,n_epoch=30,n_batch=20,verbose = False)\n",
    "\n",
    "clf4.fit(X_train,y_train)\n",
    "y_pred4 = clf4.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred4)\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid,axis=1), y_pred4)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各エポックの損失関数を可視化\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS')\n",
    "plt.plot(clf4.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACC')\n",
    "plt.plot(clf4.log_acc,'rs--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN5 = {0:FC(784, 400, XavierInitializer(), AdaGrad(0.01), Tanh()),\n",
    "       1:FC(400, 300, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       2:FC(300, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       3:FC(200, 100, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "       4:FC(100, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 = ScratchDNNClassifier(NN=NN5,n_epoch=30,n_batch=20,verbose = False)\n",
    "\n",
    "clf5.fit(X_train,y_train)\n",
    "y_pred5 = clf5.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred5)\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_valid,axis=1), y_pred5)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各エポックの損失関数を可視化\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS')\n",
    "plt.plot(clf5.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACC')\n",
    "plt.plot(clf5.log_acc,'rs--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagradをインスタンス化するだけで十分か\n",
    "class TestScratchDeepNeuralNetworkClassifier():\n",
    "    \"\"\"\n",
    "    N層ニューラルネットワーク分類器\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.sigma : ガウス分布の標準偏差\n",
    "    self.lr : 学習率\n",
    "    self.n_nodes1 : 第1層のノード数\n",
    "    self.n_nodes2 : 第2層のノード数\n",
    "    self.n_output : 出力層のノード数\n",
    "    \n",
    "    self.n_epoch : エポック数\n",
    "    self.n_batch : バッチ数\n",
    "    self.verbose : 学習プロセスの可視化\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes1, n_nodes2, n_output,\n",
    "                 sigma, n_epoch, n_batch, lr, verbose = False):\n",
    "        # Parameters\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "                \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        \"\"\"\n",
    "        ニューラル ネットワーク分類器をトレーニング\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            訓練データの特徴\n",
    "        y : 次の形の ndarray, shape (n_samples, )\n",
    "            学習データの正解値\n",
    "        X_val : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            検証データの特徴\n",
    "        y_val : 次の形の ndarray, shape (n_samples, )\n",
    "            検証データの正しい値\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer = AdaGrad(self.lr)\n",
    "    \n",
    "        initializer1 = XavierInitializer()\n",
    "        initializer2 = XavierInitializer()\n",
    "        initializer3 = SimpleInitializer(self.sigma)\n",
    "        \n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer, Tanh())\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer, Tanh())\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer, Softmax())\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            \n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                # フォワードプロパゲーション\n",
    "                self.Z1 = self.FC1.forward(mini_X_train)\n",
    "                self.Z2 = self.FC2.forward(self.Z1)\n",
    "                self.Z3 = self.FC3.forward(self.Z2)    \n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                self.dA3 = self.Z3 - mini_y_train\n",
    "                self.dZ2 = self.FC3.backward(self.dA3)\n",
    "                self.dZ1 = self.FC2.backward(self.dZ2)\n",
    "                self.dZ0 = self.FC1.backward(self.dZ1)\n",
    "                \n",
    "                # 損失関数\n",
    "                self.loss += self.loss_function(self.Z3,mini_y_train)\n",
    "                \n",
    "            self.log_loss[epoch] = self.loss/self.n_batch\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラル ネットワーク分類器を使用して推定\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形の ndarray、形状 (n_samples、n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形の ndarray、形状 (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        pred_Z1 = self.FC1.forward(X)\n",
    "        pred_Z2 = self.FC2.forward(pred_Z1)\n",
    "        return np.argmax(self.FC3.forward(pred_Z2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = TestScratchDeepNeuralNetworkClassifier(n_epoch=2, n_features=784,\n",
    "                                             n_nodes1=400, n_nodes2=200,\n",
    "                                             n_output=10, sigma=0.01, n_batch=100,\n",
    "                                             lr = 0.01, verbose = False)\n",
    "\n",
    "tst.fit(X_train,y_train)\n",
    "y_pred = tst.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bde1ea82a499da0f3bee45254b930e19212c5f0c991811db65179495d8206895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
